{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import library as l\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_previous_data(indices, file='aggressive_dedup.json'):\n",
    "    data = defaultdict(list)\n",
    "    time = defaultdict(list)\n",
    "    #ratings = defaultdict(list)\n",
    "    \n",
    "    idx = sorted([item for sublist in indices.values() for item in sublist])\n",
    "    x = sorted(list(indices.keys()))\n",
    "    for yr in x:\n",
    "        data[yr] = list(np.zeros(len(indices[yr])))\n",
    "        time[yr] = list(np.zeros(len(indices[yr])))\n",
    "        #ratings[yr] = list(np.zeros(len(indices[yr])))\n",
    "    with open(file) as infile:\n",
    "        i = 0\n",
    "        for line in infile:\n",
    "            if i == idx[0]:\n",
    "                idx.pop(0)\n",
    "                x = json.loads(line)\n",
    "                yr = x['reviewTime'][-4:]\n",
    "                j = indices[yr].index(i)\n",
    "                data[yr][j] = x.get('reviewText')\n",
    "                time[yr][j] = x.get('reviewTime')\n",
    "                #ratings[yr][j] = int(x.get('overall'))    \n",
    "            i += 1\n",
    "            if len(idx) == 0:\n",
    "                break\n",
    "    \n",
    "    return data, time#, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadstring = []\n",
    "loadstring.append('seed_1828_size_40000_topsize_300_date_2018-05-18')\n",
    "loadstring.append('seed_16376_size_40000_topsize_300_date_2018-05-18')\n",
    "loadstring.append('seed_23662_size_40000_topsize_300_date_2018-05-18')\n",
    "\n",
    "loop = range(len(loadstring))\n",
    "\n",
    "start_year = 2001\n",
    "end_year = 2014\n",
    "\n",
    "main = {}\n",
    "\n",
    "for j, lstr in enumerate(loadstring):\n",
    "    main[j] = {}\n",
    "\n",
    "    main[j]['test_idx'] = {}\n",
    "    for i, indx in enumerate(['/test_idx_' + str(start_year+x) + '.data' for x in range(end_year - start_year + 1)]):\n",
    "        main[j]['test_idx'][i+1] = l.load_file(lstr + indx)\n",
    "\n",
    "    main[j]['train_idx'] = {}\n",
    "    for i, indx in enumerate(['/train_idx_' + str(start_year+x) + '.data' for x in range(end_year - start_year + 1)]):\n",
    "        main[j]['train_idx'][i+1] = l.load_file(lstr + indx)\n",
    "\n",
    "    main[j]['indices'] = l.load_file(lstr + '/indices.data')\n",
    "    main[j]['ratings'] = l.load_file(lstr + '/ratings.data')\n",
    "\n",
    "for z in loop:\n",
    "    main[z]['data'], main[z]['time'] = load_previous_data(main[z]['indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savestring = []\n",
    "savestring.append('seed_1828_size_40000_topsize_300_date_2018-05-18_BETTER')\n",
    "savestring.append('seed_16376_size_40000_topsize_300_date_2018-05-18_BETTER')\n",
    "savestring.append('seed_23662_size_40000_topsize_300_date_2018-05-18_BETTER')\n",
    "\n",
    "LR = LogisticRegression(penalty='l1', n_jobs=-1, solver='saga', max_iter=300)\n",
    "CV = CountVectorizer(ngram_range=(1,2), analyzer='word', min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-51e5fe5258b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mrat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ratings'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msavestring\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2012-2014'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = [*loop]\n",
    "y_test = [*loop]\n",
    "\n",
    "for z in loop:\n",
    "    arr = []\n",
    "    rat = []\n",
    "    for key in range(12,15):\n",
    "        arr.append(np.array(main[z]['data'][str(2000 + key)]))\n",
    "        rat.append(np.array(main[z]['ratings'][str(2000 + key)]))\n",
    "    _, _, y_train[z], y_test[z] = l.test_train(arr, rat, savestring[z], '2012-2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4981bd384372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ratings'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2001-2004'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'seed_1828_size_40000_topsize_300_date_2018-05-18_BETTER'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_scores' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = [*loop]\n",
    "X_test = [*loop]\n",
    "\n",
    "for z in loop:\n",
    "    arr = []\n",
    "    rat = []\n",
    "    for key in range(1,5):\n",
    "        arr.append(np.array(main[z]['data'][str(2000 + key)]))\n",
    "        rat.append(np.array(main[z]['ratings'][str(2000 + key)]))\n",
    "        \n",
    "    X_train[z], X_test[z], _, _ = l.test_train(arr, rat, savestring[z], '2001-2004')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sorted(main[0]['models'].keys())\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for i, each in enumerate(x):\n",
    "    tempo = []\n",
    "    for z in loop:\n",
    "        temp = []\n",
    "        #arr = np.array(main[z]['data'][str(2000 + each)])\n",
    "        #X_train, X_test = arr[main[z]['train_idx'][each]], arr[main[z]['test_idx'][each]]\n",
    "        \n",
    "        X_test = np.array(main[z]['data'][str(2000 + each)])[main[z]['test_idx'][each]]\n",
    "        \n",
    "        #arr = np.array(main[z]['ratings'][str(2000 + each)])\n",
    "        #y_train, y_test = arr[main[z]['train_idx'][each]], arr[main[z]['test_idx'][each]]\n",
    "        \n",
    "        #X_train = main[z]['CV'].transform(X_train)\n",
    "        \n",
    "        X_test = main[z]['CV'].transform(X_test)\n",
    "        y_test = np.array(main[z]['ratings'][str(2000 + each)])[main[z]['test_idx'][each]]\n",
    "        \n",
    "        temp.append(main[z]['models'][each].score(X_test, y_test))\n",
    "\n",
    "        print('Trained on: ' + str(2000 + each) + '  Scored on: ' + str(2000 + each) + '    acc: ', temp[-1])\n",
    "        \n",
    "        for every in x[i+1:]:\n",
    "            #arr = np.array(main[z]['data'][str(2000 + every)])\n",
    "            #X_train, X_test = arr[main[z]['train_idx'][every]], arr[main[z]['test_idx'][every]]\n",
    "            \n",
    "            X_test = np.array(main[z]['data'][str(2000 + every)])[main[z]['test_idx'][every]]\n",
    "            \n",
    "            #arr = np.array(main[z]['ratings'][str(2000 + every)])\n",
    "            #y_train, y_test = arr[main[z]['train_idx'][every]], arr[main[z]['test_idx'][every]]\n",
    "\n",
    "            #X_train = main[z]['CV'].transform(X_train)\n",
    "            X_test = main[z]['CV'].transform(X_test)\n",
    "            y_test = np.array(main[z]['ratings'][str(2000 + every)])[main[z]['test_idx'][every]]\n",
    "            \n",
    "            temp.append(main[z]['models'][each].score(X_test, y_test))\n",
    "            \n",
    "            print('Trained on: ' + str(2000 + each) + '  Scored on: ' + str(2000 + every) + '    acc: ', temp[-1])\n",
    "        print()\n",
    "        tempo.append(temp)\n",
    "    results.append(tempo)\n",
    "    if i == len(x) - 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
