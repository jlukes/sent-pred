{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import library as l\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "D:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.19.0 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "'''#loadstring = 'seed_1519_size_20000_topsize_300_date_2018-04-12'\n",
    "loadstring = 'seed_39169_size_40000_topsize_300_date_2018-04-12'\n",
    "\n",
    "models = {}\n",
    "for i, model in enumerate(['/LR_' + str(2001+x) + '.data' for x in range(14)]):\n",
    "    models[i+1] = l.load_file(loadstring + model)\n",
    "\n",
    "top_f = l.load_file(loadstring + '/top_f.data')\n",
    "bot_f = l.load_file(loadstring + '/top_f.data')\n",
    "CV = l.load_file(loadstring + '/CV_2001.data')\n",
    "indices = l.load_file(loadstring + '/indices.data')\n",
    "ratings = l.load_file(loadstring + '/ratings.data')\n",
    "seen = l.load_file(loadstring + '/seen.data')\n",
    "test_idx = l.load_file(loadstring + '/test_idx.data')\n",
    "train_idx = l.load_file(loadstring + '/train_idx.data')\n",
    "features = list(CV.get_feature_names())\n",
    "print(len(features))'''\n",
    "\n",
    "loadstring = []\n",
    "loadstring.append('seed_6676_size_40000_topsize_300_date_2018-04-23')\n",
    "loadstring.append('seed_15320_size_40000_topsize_300_date_2018-05-04')\n",
    "loadstring.append('seed_39729_size_40000_topsize_300_date_2018-05-04')\n",
    "\n",
    "loop = range(len(loadstring))\n",
    "\n",
    "start_year = 2001\n",
    "end_year = 2014\n",
    "\n",
    "main = {}\n",
    "\n",
    "for j, lstr in enumerate(loadstring):\n",
    "    main[j] = {}\n",
    "\n",
    "    main[j]['models'] = {}\n",
    "    for i, model in enumerate(['/LR_' + str(start_year+x) + '.data' for x in range(end_year - start_year + 1)]):\n",
    "        main[j]['models'][i+1] = l.load_file(lstr + model)\n",
    "\n",
    "    main[j]['test_idx'] = {}\n",
    "    for i, indx in enumerate(['/test_idx_' + str(start_year+x) + '.data' for x in range(end_year - start_year + 1)]):\n",
    "        main[j]['test_idx'][i+1] = l.load_file(lstr + indx)\n",
    "\n",
    "    main[j]['train_idx'] = {}\n",
    "    for i, indx in enumerate(['/train_idx_' + str(start_year+x) + '.data' for x in range(end_year - start_year + 1)]):\n",
    "        main[j]['train_idx'][i+1] = l.load_file(lstr + indx)\n",
    "\n",
    "    main[j]['top_f'] = l.load_file(lstr + '/top_f.data')\n",
    "    main[j]['bot_f'] = l.load_file(lstr + '/top_f.data')\n",
    "    main[j]['CV'] = l.load_file(lstr + '/CV_2001.data')\n",
    "    main[j]['indices'] = l.load_file(lstr + '/indices.data')\n",
    "    main[j]['ratings'] = l.load_file(lstr + '/ratings.data')\n",
    "    main[j]['seen'] = l.load_file(lstr + '/seen.data')\n",
    "    main[j]['features'] = list(main[j]['CV'].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "def load_previous_data(indices, file='aggressive_dedup.json'):\n",
    "    data = defaultdict(list)\n",
    "    time = defaultdict(list)\n",
    "    #ratings = defaultdict(list)\n",
    "    \n",
    "    idx = sorted([item for sublist in indices.values() for item in sublist])\n",
    "    x = sorted(list(indices.keys()))\n",
    "    for yr in x:\n",
    "        data[yr] = list(np.zeros(len(indices[yr])))\n",
    "        time[yr] = list(np.zeros(len(indices[yr])))\n",
    "        #ratings[yr] = list(np.zeros(len(indices[yr])))\n",
    "    with open(file) as infile:\n",
    "        i = 0\n",
    "        for line in infile:\n",
    "            if i == idx[0]:\n",
    "                idx.pop(0)\n",
    "                x = json.loads(line)\n",
    "                yr = x['reviewTime'][-4:]\n",
    "                j = indices[yr].index(i)\n",
    "                data[yr][j] = x.get('reviewText')\n",
    "                time[yr][j] = x.get('reviewTime')\n",
    "                #ratings[yr][j] = int(x.get('overall'))    \n",
    "            i += 1\n",
    "            if len(idx) == 0:\n",
    "                break\n",
    "    \n",
    "    return data, time#, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for z in loop:\n",
    "    main[z]['data'], main[z]['time'] = load_previous_data(main[z]['indices'])\n",
    "    #l.simplify_ratings(main[z]['data'], main[z]['ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2010', '2003', '2007', '2014', '2008', '2004', '2001', '2011', '2005', '2009', '2006', '2013', '2002', '2012'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main[z]['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2010', '2003', '2013', '2014', '2008', '2004', '2001', '2011', '2005', '2009', '2006', '2007', '2002', '2012'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main[z]['ratings'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on: 2001  Scored on: 2001    acc:  0.879625\n",
      "Trained on: 2001  Scored on: 2002    acc:  0.875375\n",
      "Trained on: 2001  Scored on: 2003    acc:  0.863375\n",
      "Trained on: 2001  Scored on: 2004    acc:  0.855125\n",
      "Trained on: 2001  Scored on: 2005    acc:  0.854875\n",
      "Trained on: 2001  Scored on: 2006    acc:  0.862625\n",
      "Trained on: 2001  Scored on: 2007    acc:  0.859\n",
      "Trained on: 2001  Scored on: 2008    acc:  0.8485\n",
      "Trained on: 2001  Scored on: 2009    acc:  0.85375\n",
      "Trained on: 2001  Scored on: 2010    acc:  0.843875\n",
      "Trained on: 2001  Scored on: 2011    acc:  0.834875\n",
      "Trained on: 2001  Scored on: 2012    acc:  0.861125\n",
      "Trained on: 2001  Scored on: 2013    acc:  0.858125\n",
      "Trained on: 2001  Scored on: 2014    acc:  0.85075\n",
      "\n",
      "Trained on: 2001  Scored on: 2001    acc:  0.8785\n",
      "Trained on: 2001  Scored on: 2002    acc:  0.8775\n",
      "Trained on: 2001  Scored on: 2003    acc:  0.86625\n",
      "Trained on: 2001  Scored on: 2004    acc:  0.854875\n",
      "Trained on: 2001  Scored on: 2005    acc:  0.856375\n",
      "Trained on: 2001  Scored on: 2006    acc:  0.8545\n",
      "Trained on: 2001  Scored on: 2007    acc:  0.856125\n",
      "Trained on: 2001  Scored on: 2008    acc:  0.849875\n",
      "Trained on: 2001  Scored on: 2009    acc:  0.852125\n",
      "Trained on: 2001  Scored on: 2010    acc:  0.84925\n",
      "Trained on: 2001  Scored on: 2011    acc:  0.84325\n",
      "Trained on: 2001  Scored on: 2012    acc:  0.86025\n",
      "Trained on: 2001  Scored on: 2013    acc:  0.860125\n",
      "Trained on: 2001  Scored on: 2014    acc:  0.85425\n",
      "\n",
      "Trained on: 2001  Scored on: 2001    acc:  0.873125\n",
      "Trained on: 2001  Scored on: 2002    acc:  0.872\n",
      "Trained on: 2001  Scored on: 2003    acc:  0.864375\n",
      "Trained on: 2001  Scored on: 2004    acc:  0.85275\n",
      "Trained on: 2001  Scored on: 2005    acc:  0.84825\n",
      "Trained on: 2001  Scored on: 2006    acc:  0.857\n",
      "Trained on: 2001  Scored on: 2007    acc:  0.86575\n",
      "Trained on: 2001  Scored on: 2008    acc:  0.8535\n",
      "Trained on: 2001  Scored on: 2009    acc:  0.849375\n",
      "Trained on: 2001  Scored on: 2010    acc:  0.8505\n",
      "Trained on: 2001  Scored on: 2011    acc:  0.841625\n",
      "Trained on: 2001  Scored on: 2012    acc:  0.851375\n",
      "Trained on: 2001  Scored on: 2013    acc:  0.854875\n",
      "Trained on: 2001  Scored on: 2014    acc:  0.84525\n",
      "\n",
      "Trained on: 2002  Scored on: 2002    acc:  0.867625\n",
      "Trained on: 2002  Scored on: 2003    acc:  0.862125\n",
      "Trained on: 2002  Scored on: 2004    acc:  0.857375\n",
      "Trained on: 2002  Scored on: 2005    acc:  0.852625\n",
      "Trained on: 2002  Scored on: 2006    acc:  0.861125\n",
      "Trained on: 2002  Scored on: 2007    acc:  0.864125\n",
      "Trained on: 2002  Scored on: 2008    acc:  0.854625\n",
      "Trained on: 2002  Scored on: 2009    acc:  0.85425\n",
      "Trained on: 2002  Scored on: 2010    acc:  0.8505\n",
      "Trained on: 2002  Scored on: 2011    acc:  0.8395\n",
      "Trained on: 2002  Scored on: 2012    acc:  0.8655\n",
      "Trained on: 2002  Scored on: 2013    acc:  0.858\n",
      "Trained on: 2002  Scored on: 2014    acc:  0.855\n",
      "\n",
      "Trained on: 2002  Scored on: 2002    acc:  0.871875\n",
      "Trained on: 2002  Scored on: 2003    acc:  0.853375\n",
      "Trained on: 2002  Scored on: 2004    acc:  0.844125\n",
      "Trained on: 2002  Scored on: 2005    acc:  0.8395\n",
      "Trained on: 2002  Scored on: 2006    acc:  0.840875\n",
      "Trained on: 2002  Scored on: 2007    acc:  0.847\n",
      "Trained on: 2002  Scored on: 2008    acc:  0.84075\n",
      "Trained on: 2002  Scored on: 2009    acc:  0.84175\n",
      "Trained on: 2002  Scored on: 2010    acc:  0.833\n",
      "Trained on: 2002  Scored on: 2011    acc:  0.841125\n",
      "Trained on: 2002  Scored on: 2012    acc:  0.84725\n",
      "Trained on: 2002  Scored on: 2013    acc:  0.849\n",
      "Trained on: 2002  Scored on: 2014    acc:  0.84375\n",
      "\n",
      "Trained on: 2002  Scored on: 2002    acc:  0.879\n",
      "Trained on: 2002  Scored on: 2003    acc:  0.872125\n",
      "Trained on: 2002  Scored on: 2004    acc:  0.8625\n",
      "Trained on: 2002  Scored on: 2005    acc:  0.859125\n",
      "Trained on: 2002  Scored on: 2006    acc:  0.8585\n",
      "Trained on: 2002  Scored on: 2007    acc:  0.87375\n",
      "Trained on: 2002  Scored on: 2008    acc:  0.860125\n",
      "Trained on: 2002  Scored on: 2009    acc:  0.855375\n",
      "Trained on: 2002  Scored on: 2010    acc:  0.85525\n",
      "Trained on: 2002  Scored on: 2011    acc:  0.8485\n",
      "Trained on: 2002  Scored on: 2012    acc:  0.860375\n",
      "Trained on: 2002  Scored on: 2013    acc:  0.86425\n",
      "Trained on: 2002  Scored on: 2014    acc:  0.84975\n",
      "\n",
      "Trained on: 2003  Scored on: 2003    acc:  0.868625\n",
      "Trained on: 2003  Scored on: 2004    acc:  0.86325\n",
      "Trained on: 2003  Scored on: 2005    acc:  0.8525\n",
      "Trained on: 2003  Scored on: 2006    acc:  0.868\n",
      "Trained on: 2003  Scored on: 2007    acc:  0.862875\n",
      "Trained on: 2003  Scored on: 2008    acc:  0.858\n",
      "Trained on: 2003  Scored on: 2009    acc:  0.854875\n",
      "Trained on: 2003  Scored on: 2010    acc:  0.85125\n",
      "Trained on: 2003  Scored on: 2011    acc:  0.850875\n",
      "Trained on: 2003  Scored on: 2012    acc:  0.871375\n",
      "Trained on: 2003  Scored on: 2013    acc:  0.867\n",
      "Trained on: 2003  Scored on: 2014    acc:  0.86425\n",
      "\n",
      "Trained on: 2003  Scored on: 2003    acc:  0.87175\n",
      "Trained on: 2003  Scored on: 2004    acc:  0.8615\n",
      "Trained on: 2003  Scored on: 2005    acc:  0.85675\n",
      "Trained on: 2003  Scored on: 2006    acc:  0.85525\n",
      "Trained on: 2003  Scored on: 2007    acc:  0.86225\n",
      "Trained on: 2003  Scored on: 2008    acc:  0.856875\n",
      "Trained on: 2003  Scored on: 2009    acc:  0.8545\n",
      "Trained on: 2003  Scored on: 2010    acc:  0.847875\n",
      "Trained on: 2003  Scored on: 2011    acc:  0.85\n",
      "Trained on: 2003  Scored on: 2012    acc:  0.85725\n",
      "Trained on: 2003  Scored on: 2013    acc:  0.858375\n",
      "Trained on: 2003  Scored on: 2014    acc:  0.854625\n",
      "\n",
      "Trained on: 2003  Scored on: 2003    acc:  0.869125\n",
      "Trained on: 2003  Scored on: 2004    acc:  0.856625\n",
      "Trained on: 2003  Scored on: 2005    acc:  0.86225\n",
      "Trained on: 2003  Scored on: 2006    acc:  0.863625\n",
      "Trained on: 2003  Scored on: 2007    acc:  0.871375\n",
      "Trained on: 2003  Scored on: 2008    acc:  0.859375\n",
      "Trained on: 2003  Scored on: 2009    acc:  0.857375\n",
      "Trained on: 2003  Scored on: 2010    acc:  0.85575\n",
      "Trained on: 2003  Scored on: 2011    acc:  0.847375\n",
      "Trained on: 2003  Scored on: 2012    acc:  0.85825\n",
      "Trained on: 2003  Scored on: 2013    acc:  0.866625\n",
      "Trained on: 2003  Scored on: 2014    acc:  0.853625\n",
      "\n",
      "Trained on: 2004  Scored on: 2004    acc:  0.856625\n",
      "Trained on: 2004  Scored on: 2005    acc:  0.854125\n",
      "Trained on: 2004  Scored on: 2006    acc:  0.86425\n",
      "Trained on: 2004  Scored on: 2007    acc:  0.859875\n",
      "Trained on: 2004  Scored on: 2008    acc:  0.857125\n",
      "Trained on: 2004  Scored on: 2009    acc:  0.850625\n",
      "Trained on: 2004  Scored on: 2010    acc:  0.85075\n",
      "Trained on: 2004  Scored on: 2011    acc:  0.843\n",
      "Trained on: 2004  Scored on: 2012    acc:  0.86775\n",
      "Trained on: 2004  Scored on: 2013    acc:  0.86275\n",
      "Trained on: 2004  Scored on: 2014    acc:  0.85675\n",
      "\n",
      "Trained on: 2004  Scored on: 2004    acc:  0.854625\n",
      "Trained on: 2004  Scored on: 2005    acc:  0.847875\n",
      "Trained on: 2004  Scored on: 2006    acc:  0.847625\n",
      "Trained on: 2004  Scored on: 2007    acc:  0.852\n",
      "Trained on: 2004  Scored on: 2008    acc:  0.84975\n",
      "Trained on: 2004  Scored on: 2009    acc:  0.851625\n",
      "Trained on: 2004  Scored on: 2010    acc:  0.844\n",
      "Trained on: 2004  Scored on: 2011    acc:  0.841875\n",
      "Trained on: 2004  Scored on: 2012    acc:  0.854875\n",
      "Trained on: 2004  Scored on: 2013    acc:  0.85775\n",
      "Trained on: 2004  Scored on: 2014    acc:  0.855875\n",
      "\n",
      "Trained on: 2004  Scored on: 2004    acc:  0.855875\n",
      "Trained on: 2004  Scored on: 2005    acc:  0.85725\n",
      "Trained on: 2004  Scored on: 2006    acc:  0.857125\n",
      "Trained on: 2004  Scored on: 2007    acc:  0.862875\n",
      "Trained on: 2004  Scored on: 2008    acc:  0.85725\n",
      "Trained on: 2004  Scored on: 2009    acc:  0.852375\n",
      "Trained on: 2004  Scored on: 2010    acc:  0.848625\n",
      "Trained on: 2004  Scored on: 2011    acc:  0.84175\n",
      "Trained on: 2004  Scored on: 2012    acc:  0.852375\n",
      "Trained on: 2004  Scored on: 2013    acc:  0.85875\n",
      "Trained on: 2004  Scored on: 2014    acc:  0.845625\n",
      "\n",
      "Trained on: 2005  Scored on: 2005    acc:  0.850375\n",
      "Trained on: 2005  Scored on: 2006    acc:  0.858625\n",
      "Trained on: 2005  Scored on: 2007    acc:  0.858125\n",
      "Trained on: 2005  Scored on: 2008    acc:  0.8585\n",
      "Trained on: 2005  Scored on: 2009    acc:  0.853\n",
      "Trained on: 2005  Scored on: 2010    acc:  0.851\n",
      "Trained on: 2005  Scored on: 2011    acc:  0.84375\n",
      "Trained on: 2005  Scored on: 2012    acc:  0.866875\n",
      "Trained on: 2005  Scored on: 2013    acc:  0.864\n",
      "Trained on: 2005  Scored on: 2014    acc:  0.857625\n",
      "\n",
      "Trained on: 2005  Scored on: 2005    acc:  0.841625\n",
      "Trained on: 2005  Scored on: 2006    acc:  0.844875\n",
      "Trained on: 2005  Scored on: 2007    acc:  0.847125\n",
      "Trained on: 2005  Scored on: 2008    acc:  0.85025\n",
      "Trained on: 2005  Scored on: 2009    acc:  0.849\n",
      "Trained on: 2005  Scored on: 2010    acc:  0.84475\n",
      "Trained on: 2005  Scored on: 2011    acc:  0.845375\n",
      "Trained on: 2005  Scored on: 2012    acc:  0.85225\n",
      "Trained on: 2005  Scored on: 2013    acc:  0.855125\n",
      "Trained on: 2005  Scored on: 2014    acc:  0.851625\n",
      "\n",
      "Trained on: 2005  Scored on: 2005    acc:  0.845625\n",
      "Trained on: 2005  Scored on: 2006    acc:  0.844375\n",
      "Trained on: 2005  Scored on: 2007    acc:  0.85675\n",
      "Trained on: 2005  Scored on: 2008    acc:  0.85225\n",
      "Trained on: 2005  Scored on: 2009    acc:  0.847625\n",
      "Trained on: 2005  Scored on: 2010    acc:  0.846\n",
      "Trained on: 2005  Scored on: 2011    acc:  0.83825\n",
      "Trained on: 2005  Scored on: 2012    acc:  0.849875\n",
      "Trained on: 2005  Scored on: 2013    acc:  0.85825\n",
      "Trained on: 2005  Scored on: 2014    acc:  0.844875\n",
      "\n",
      "Trained on: 2006  Scored on: 2006    acc:  0.8475\n",
      "Trained on: 2006  Scored on: 2007    acc:  0.845375\n",
      "Trained on: 2006  Scored on: 2008    acc:  0.84375\n",
      "Trained on: 2006  Scored on: 2009    acc:  0.844125\n",
      "Trained on: 2006  Scored on: 2010    acc:  0.841375\n",
      "Trained on: 2006  Scored on: 2011    acc:  0.834375\n",
      "Trained on: 2006  Scored on: 2012    acc:  0.858125\n",
      "Trained on: 2006  Scored on: 2013    acc:  0.856625\n",
      "Trained on: 2006  Scored on: 2014    acc:  0.84575\n",
      "\n",
      "Trained on: 2006  Scored on: 2006    acc:  0.8525\n",
      "Trained on: 2006  Scored on: 2007    acc:  0.860125\n",
      "Trained on: 2006  Scored on: 2008    acc:  0.858625\n",
      "Trained on: 2006  Scored on: 2009    acc:  0.861125\n",
      "Trained on: 2006  Scored on: 2010    acc:  0.855\n",
      "Trained on: 2006  Scored on: 2011    acc:  0.855125\n",
      "Trained on: 2006  Scored on: 2012    acc:  0.86825\n",
      "Trained on: 2006  Scored on: 2013    acc:  0.8685\n",
      "Trained on: 2006  Scored on: 2014    acc:  0.868375\n",
      "\n",
      "Trained on: 2006  Scored on: 2006    acc:  0.858375\n",
      "Trained on: 2006  Scored on: 2007    acc:  0.8685\n",
      "Trained on: 2006  Scored on: 2008    acc:  0.857625\n",
      "Trained on: 2006  Scored on: 2009    acc:  0.851375\n",
      "Trained on: 2006  Scored on: 2010    acc:  0.8525\n",
      "Trained on: 2006  Scored on: 2011    acc:  0.8535\n",
      "Trained on: 2006  Scored on: 2012    acc:  0.8585\n",
      "Trained on: 2006  Scored on: 2013    acc:  0.868875\n",
      "Trained on: 2006  Scored on: 2014    acc:  0.85775\n",
      "\n",
      "Trained on: 2007  Scored on: 2007    acc:  0.84725\n",
      "Trained on: 2007  Scored on: 2008    acc:  0.846625\n",
      "Trained on: 2007  Scored on: 2009    acc:  0.841625\n",
      "Trained on: 2007  Scored on: 2010    acc:  0.839875\n",
      "Trained on: 2007  Scored on: 2011    acc:  0.838875\n",
      "Trained on: 2007  Scored on: 2012    acc:  0.86175\n",
      "Trained on: 2007  Scored on: 2013    acc:  0.86125\n",
      "Trained on: 2007  Scored on: 2014    acc:  0.85425\n",
      "\n",
      "Trained on: 2007  Scored on: 2007    acc:  0.856\n",
      "Trained on: 2007  Scored on: 2008    acc:  0.855875\n",
      "Trained on: 2007  Scored on: 2009    acc:  0.85175\n",
      "Trained on: 2007  Scored on: 2010    acc:  0.851125\n",
      "Trained on: 2007  Scored on: 2011    acc:  0.8555\n",
      "Trained on: 2007  Scored on: 2012    acc:  0.863\n",
      "Trained on: 2007  Scored on: 2013    acc:  0.87\n",
      "Trained on: 2007  Scored on: 2014    acc:  0.868125\n",
      "\n",
      "Trained on: 2007  Scored on: 2007    acc:  0.857125\n",
      "Trained on: 2007  Scored on: 2008    acc:  0.854\n",
      "Trained on: 2007  Scored on: 2009    acc:  0.846875\n",
      "Trained on: 2007  Scored on: 2010    acc:  0.847875\n",
      "Trained on: 2007  Scored on: 2011    acc:  0.84475\n",
      "Trained on: 2007  Scored on: 2012    acc:  0.854\n",
      "Trained on: 2007  Scored on: 2013    acc:  0.86875\n",
      "Trained on: 2007  Scored on: 2014    acc:  0.854125\n",
      "\n",
      "Trained on: 2008  Scored on: 2008    acc:  0.85725\n",
      "Trained on: 2008  Scored on: 2009    acc:  0.857125\n",
      "Trained on: 2008  Scored on: 2010    acc:  0.854875\n",
      "Trained on: 2008  Scored on: 2011    acc:  0.854375\n",
      "Trained on: 2008  Scored on: 2012    acc:  0.875875\n",
      "Trained on: 2008  Scored on: 2013    acc:  0.87475\n",
      "Trained on: 2008  Scored on: 2014    acc:  0.867\n",
      "\n",
      "Trained on: 2008  Scored on: 2008    acc:  0.856625\n",
      "Trained on: 2008  Scored on: 2009    acc:  0.858375\n",
      "Trained on: 2008  Scored on: 2010    acc:  0.85625\n",
      "Trained on: 2008  Scored on: 2011    acc:  0.859625\n",
      "Trained on: 2008  Scored on: 2012    acc:  0.869875\n",
      "Trained on: 2008  Scored on: 2013    acc:  0.871375\n",
      "Trained on: 2008  Scored on: 2014    acc:  0.876125\n",
      "\n",
      "Trained on: 2008  Scored on: 2008    acc:  0.852375\n",
      "Trained on: 2008  Scored on: 2009    acc:  0.842875\n",
      "Trained on: 2008  Scored on: 2010    acc:  0.846625\n",
      "Trained on: 2008  Scored on: 2011    acc:  0.841875\n",
      "Trained on: 2008  Scored on: 2012    acc:  0.853375\n",
      "Trained on: 2008  Scored on: 2013    acc:  0.863375\n",
      "Trained on: 2008  Scored on: 2014    acc:  0.853875\n",
      "\n",
      "Trained on: 2009  Scored on: 2009    acc:  0.860375\n",
      "Trained on: 2009  Scored on: 2010    acc:  0.8625\n",
      "Trained on: 2009  Scored on: 2011    acc:  0.8575\n",
      "Trained on: 2009  Scored on: 2012    acc:  0.87475\n",
      "Trained on: 2009  Scored on: 2013    acc:  0.8775\n",
      "Trained on: 2009  Scored on: 2014    acc:  0.87225\n",
      "\n",
      "Trained on: 2009  Scored on: 2009    acc:  0.841625\n",
      "Trained on: 2009  Scored on: 2010    acc:  0.84075\n",
      "Trained on: 2009  Scored on: 2011    acc:  0.84425\n",
      "Trained on: 2009  Scored on: 2012    acc:  0.853625\n",
      "Trained on: 2009  Scored on: 2013    acc:  0.859125\n",
      "Trained on: 2009  Scored on: 2014    acc:  0.85925\n",
      "\n",
      "Trained on: 2009  Scored on: 2009    acc:  0.83675\n",
      "Trained on: 2009  Scored on: 2010    acc:  0.838375\n",
      "Trained on: 2009  Scored on: 2011    acc:  0.83525\n",
      "Trained on: 2009  Scored on: 2012    acc:  0.846625\n",
      "Trained on: 2009  Scored on: 2013    acc:  0.863625\n",
      "Trained on: 2009  Scored on: 2014    acc:  0.84575\n",
      "\n",
      "Trained on: 2010  Scored on: 2010    acc:  0.844375\n",
      "Trained on: 2010  Scored on: 2011    acc:  0.842\n",
      "Trained on: 2010  Scored on: 2012    acc:  0.86075\n",
      "Trained on: 2010  Scored on: 2013    acc:  0.866625\n",
      "Trained on: 2010  Scored on: 2014    acc:  0.855875\n",
      "\n",
      "Trained on: 2010  Scored on: 2010    acc:  0.847125\n",
      "Trained on: 2010  Scored on: 2011    acc:  0.850375\n",
      "Trained on: 2010  Scored on: 2012    acc:  0.855875\n",
      "Trained on: 2010  Scored on: 2013    acc:  0.86325\n",
      "Trained on: 2010  Scored on: 2014    acc:  0.862875\n",
      "\n",
      "Trained on: 2010  Scored on: 2010    acc:  0.847875\n",
      "Trained on: 2010  Scored on: 2011    acc:  0.843875\n",
      "Trained on: 2010  Scored on: 2012    acc:  0.856375\n",
      "Trained on: 2010  Scored on: 2013    acc:  0.86675\n",
      "Trained on: 2010  Scored on: 2014    acc:  0.854625\n",
      "\n",
      "Trained on: 2011  Scored on: 2011    acc:  0.846\n",
      "Trained on: 2011  Scored on: 2012    acc:  0.868125\n",
      "Trained on: 2011  Scored on: 2013    acc:  0.872125\n",
      "Trained on: 2011  Scored on: 2014    acc:  0.858875\n",
      "\n",
      "Trained on: 2011  Scored on: 2011    acc:  0.852625\n",
      "Trained on: 2011  Scored on: 2012    acc:  0.859625\n",
      "Trained on: 2011  Scored on: 2013    acc:  0.867875\n",
      "Trained on: 2011  Scored on: 2014    acc:  0.86875\n",
      "\n",
      "Trained on: 2011  Scored on: 2011    acc:  0.8555\n",
      "Trained on: 2011  Scored on: 2012    acc:  0.86525\n",
      "Trained on: 2011  Scored on: 2013    acc:  0.877875\n",
      "Trained on: 2011  Scored on: 2014    acc:  0.868125\n",
      "\n",
      "Trained on: 2012  Scored on: 2012    acc:  0.864\n",
      "Trained on: 2012  Scored on: 2013    acc:  0.868375\n",
      "Trained on: 2012  Scored on: 2014    acc:  0.86075\n",
      "\n",
      "Trained on: 2012  Scored on: 2012    acc:  0.85275\n",
      "Trained on: 2012  Scored on: 2013    acc:  0.86075\n",
      "Trained on: 2012  Scored on: 2014    acc:  0.86275\n",
      "\n",
      "Trained on: 2012  Scored on: 2012    acc:  0.87175\n",
      "Trained on: 2012  Scored on: 2013    acc:  0.883875\n",
      "Trained on: 2012  Scored on: 2014    acc:  0.870125\n",
      "\n",
      "Trained on: 2013  Scored on: 2013    acc:  0.853\n",
      "Trained on: 2013  Scored on: 2014    acc:  0.84075\n",
      "\n",
      "Trained on: 2013  Scored on: 2013    acc:  0.867875\n",
      "Trained on: 2013  Scored on: 2014    acc:  0.867875\n",
      "\n",
      "Trained on: 2013  Scored on: 2013    acc:  0.88075\n",
      "Trained on: 2013  Scored on: 2014    acc:  0.871625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = sorted(main[0]['models'].keys())\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for i, each in enumerate(x):\n",
    "    tempo = []\n",
    "    for z in loop:\n",
    "        temp = []\n",
    "        #arr = np.array(main[z]['data'][str(2000 + each)])\n",
    "        #X_train, X_test = arr[main[z]['train_idx'][each]], arr[main[z]['test_idx'][each]]\n",
    "        \n",
    "        X_test = np.array(main[z]['data'][str(2000 + each)])[main[z]['test_idx'][each]]\n",
    "        \n",
    "        #arr = np.array(main[z]['ratings'][str(2000 + each)])\n",
    "        #y_train, y_test = arr[main[z]['train_idx'][each]], arr[main[z]['test_idx'][each]]\n",
    "        \n",
    "        #X_train = main[z]['CV'].transform(X_train)\n",
    "        \n",
    "        X_test = main[z]['CV'].transform(X_test)\n",
    "        y_test = np.array(main[z]['ratings'][str(2000 + each)])[main[z]['test_idx'][each]]\n",
    "        \n",
    "        temp.append(main[z]['models'][each].score(X_test, y_test))\n",
    "\n",
    "        print('Trained on: ' + str(2000 + each) + '  Scored on: ' + str(2000 + each) + '    acc: ', temp[-1])\n",
    "        \n",
    "        for every in x[i+1:]:\n",
    "            #arr = np.array(main[z]['data'][str(2000 + every)])\n",
    "            #X_train, X_test = arr[main[z]['train_idx'][every]], arr[main[z]['test_idx'][every]]\n",
    "            \n",
    "            X_test = np.array(main[z]['data'][str(2000 + every)])[main[z]['test_idx'][every]]\n",
    "            \n",
    "            #arr = np.array(main[z]['ratings'][str(2000 + every)])\n",
    "            #y_train, y_test = arr[main[z]['train_idx'][every]], arr[main[z]['test_idx'][every]]\n",
    "\n",
    "            #X_train = main[z]['CV'].transform(X_train)\n",
    "            X_test = main[z]['CV'].transform(X_test)\n",
    "            y_test = np.array(main[z]['ratings'][str(2000 + every)])[main[z]['test_idx'][every]]\n",
    "            \n",
    "            temp.append(main[z]['models'][each].score(X_test, y_test))\n",
    "            \n",
    "            print('Trained on: ' + str(2000 + each) + '  Scored on: ' + str(2000 + every) + '    acc: ', temp[-1])\n",
    "        print()\n",
    "        tempo.append(temp)\n",
    "    results.append(tempo)\n",
    "    if i == len(x) - 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#l.save_file('run_once_results.data', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = l.load_file('run_once_results.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.87962499999999999,\n",
       "   0.87537500000000001,\n",
       "   0.863375,\n",
       "   0.85512500000000002,\n",
       "   0.85487500000000005,\n",
       "   0.86262499999999998,\n",
       "   0.85899999999999999,\n",
       "   0.84850000000000003,\n",
       "   0.85375000000000001,\n",
       "   0.84387500000000004,\n",
       "   0.83487500000000003,\n",
       "   0.86112500000000003,\n",
       "   0.85812500000000003,\n",
       "   0.85075000000000001],\n",
       "  [0.87849999999999995,\n",
       "   0.87749999999999995,\n",
       "   0.86624999999999996,\n",
       "   0.85487500000000005,\n",
       "   0.856375,\n",
       "   0.85450000000000004,\n",
       "   0.85612500000000002,\n",
       "   0.84987500000000005,\n",
       "   0.85212500000000002,\n",
       "   0.84924999999999995,\n",
       "   0.84325000000000006,\n",
       "   0.86024999999999996,\n",
       "   0.86012500000000003,\n",
       "   0.85424999999999995],\n",
       "  [0.87312500000000004,\n",
       "   0.872,\n",
       "   0.864375,\n",
       "   0.85275000000000001,\n",
       "   0.84824999999999995,\n",
       "   0.85699999999999998,\n",
       "   0.86575000000000002,\n",
       "   0.85350000000000004,\n",
       "   0.84937499999999999,\n",
       "   0.85050000000000003,\n",
       "   0.84162499999999996,\n",
       "   0.85137499999999999,\n",
       "   0.85487500000000005,\n",
       "   0.84524999999999995]],\n",
       " [[0.86762499999999998,\n",
       "   0.86212500000000003,\n",
       "   0.857375,\n",
       "   0.85262499999999997,\n",
       "   0.86112500000000003,\n",
       "   0.86412500000000003,\n",
       "   0.85462499999999997,\n",
       "   0.85424999999999995,\n",
       "   0.85050000000000003,\n",
       "   0.83950000000000002,\n",
       "   0.86550000000000005,\n",
       "   0.85799999999999998,\n",
       "   0.85499999999999998],\n",
       "  [0.87187499999999996,\n",
       "   0.85337499999999999,\n",
       "   0.84412500000000001,\n",
       "   0.83950000000000002,\n",
       "   0.84087500000000004,\n",
       "   0.84699999999999998,\n",
       "   0.84075,\n",
       "   0.84175,\n",
       "   0.83299999999999996,\n",
       "   0.84112500000000001,\n",
       "   0.84724999999999995,\n",
       "   0.84899999999999998,\n",
       "   0.84375],\n",
       "  [0.879,\n",
       "   0.87212500000000004,\n",
       "   0.86250000000000004,\n",
       "   0.85912500000000003,\n",
       "   0.85850000000000004,\n",
       "   0.87375000000000003,\n",
       "   0.86012500000000003,\n",
       "   0.855375,\n",
       "   0.85524999999999995,\n",
       "   0.84850000000000003,\n",
       "   0.860375,\n",
       "   0.86424999999999996,\n",
       "   0.84975000000000001]],\n",
       " [[0.86862499999999998,\n",
       "   0.86324999999999996,\n",
       "   0.85250000000000004,\n",
       "   0.86799999999999999,\n",
       "   0.86287499999999995,\n",
       "   0.85799999999999998,\n",
       "   0.85487500000000005,\n",
       "   0.85124999999999995,\n",
       "   0.85087500000000005,\n",
       "   0.87137500000000001,\n",
       "   0.86699999999999999,\n",
       "   0.86424999999999996],\n",
       "  [0.87175000000000002,\n",
       "   0.86150000000000004,\n",
       "   0.85675000000000001,\n",
       "   0.85524999999999995,\n",
       "   0.86224999999999996,\n",
       "   0.85687500000000005,\n",
       "   0.85450000000000004,\n",
       "   0.84787500000000005,\n",
       "   0.84999999999999998,\n",
       "   0.85724999999999996,\n",
       "   0.858375,\n",
       "   0.85462499999999997],\n",
       "  [0.86912500000000004,\n",
       "   0.85662499999999997,\n",
       "   0.86224999999999996,\n",
       "   0.86362499999999998,\n",
       "   0.87137500000000001,\n",
       "   0.859375,\n",
       "   0.857375,\n",
       "   0.85575000000000001,\n",
       "   0.84737499999999999,\n",
       "   0.85824999999999996,\n",
       "   0.86662499999999998,\n",
       "   0.85362499999999997]],\n",
       " [[0.85662499999999997,\n",
       "   0.85412500000000002,\n",
       "   0.86424999999999996,\n",
       "   0.85987499999999994,\n",
       "   0.85712500000000003,\n",
       "   0.85062499999999996,\n",
       "   0.85075000000000001,\n",
       "   0.84299999999999997,\n",
       "   0.86775000000000002,\n",
       "   0.86275000000000002,\n",
       "   0.85675000000000001],\n",
       "  [0.85462499999999997,\n",
       "   0.84787500000000005,\n",
       "   0.84762499999999996,\n",
       "   0.85199999999999998,\n",
       "   0.84975000000000001,\n",
       "   0.85162499999999997,\n",
       "   0.84399999999999997,\n",
       "   0.84187500000000004,\n",
       "   0.85487500000000005,\n",
       "   0.85775000000000001,\n",
       "   0.85587500000000005],\n",
       "  [0.85587500000000005,\n",
       "   0.85724999999999996,\n",
       "   0.85712500000000003,\n",
       "   0.86287499999999995,\n",
       "   0.85724999999999996,\n",
       "   0.85237499999999999,\n",
       "   0.84862499999999996,\n",
       "   0.84175,\n",
       "   0.85237499999999999,\n",
       "   0.85875000000000001,\n",
       "   0.84562499999999996]],\n",
       " [[0.85037499999999999,\n",
       "   0.85862499999999997,\n",
       "   0.85812500000000003,\n",
       "   0.85850000000000004,\n",
       "   0.85299999999999998,\n",
       "   0.85099999999999998,\n",
       "   0.84375,\n",
       "   0.86687499999999995,\n",
       "   0.86399999999999999,\n",
       "   0.85762499999999997],\n",
       "  [0.84162499999999996,\n",
       "   0.84487500000000004,\n",
       "   0.84712500000000002,\n",
       "   0.85024999999999995,\n",
       "   0.84899999999999998,\n",
       "   0.84475,\n",
       "   0.84537499999999999,\n",
       "   0.85224999999999995,\n",
       "   0.85512500000000002,\n",
       "   0.85162499999999997],\n",
       "  [0.84562499999999996,\n",
       "   0.84437499999999999,\n",
       "   0.85675000000000001,\n",
       "   0.85224999999999995,\n",
       "   0.84762499999999996,\n",
       "   0.84599999999999997,\n",
       "   0.83825000000000005,\n",
       "   0.84987500000000005,\n",
       "   0.85824999999999996,\n",
       "   0.84487500000000004]],\n",
       " [[0.84750000000000003,\n",
       "   0.84537499999999999,\n",
       "   0.84375,\n",
       "   0.84412500000000001,\n",
       "   0.84137499999999998,\n",
       "   0.83437499999999998,\n",
       "   0.85812500000000003,\n",
       "   0.85662499999999997,\n",
       "   0.84575],\n",
       "  [0.85250000000000004,\n",
       "   0.86012500000000003,\n",
       "   0.85862499999999997,\n",
       "   0.86112500000000003,\n",
       "   0.85499999999999998,\n",
       "   0.85512500000000002,\n",
       "   0.86824999999999997,\n",
       "   0.86850000000000005,\n",
       "   0.86837500000000001],\n",
       "  [0.858375,\n",
       "   0.86850000000000005,\n",
       "   0.85762499999999997,\n",
       "   0.85137499999999999,\n",
       "   0.85250000000000004,\n",
       "   0.85350000000000004,\n",
       "   0.85850000000000004,\n",
       "   0.86887499999999995,\n",
       "   0.85775000000000001]],\n",
       " [[0.84724999999999995,\n",
       "   0.84662499999999996,\n",
       "   0.84162499999999996,\n",
       "   0.83987500000000004,\n",
       "   0.83887500000000004,\n",
       "   0.86175000000000002,\n",
       "   0.86124999999999996,\n",
       "   0.85424999999999995],\n",
       "  [0.85599999999999998,\n",
       "   0.85587500000000005,\n",
       "   0.85175000000000001,\n",
       "   0.85112500000000002,\n",
       "   0.85550000000000004,\n",
       "   0.86299999999999999,\n",
       "   0.87,\n",
       "   0.86812500000000004],\n",
       "  [0.85712500000000003,\n",
       "   0.85399999999999998,\n",
       "   0.84687500000000004,\n",
       "   0.84787500000000005,\n",
       "   0.84475,\n",
       "   0.85399999999999998,\n",
       "   0.86875000000000002,\n",
       "   0.85412500000000002]],\n",
       " [[0.85724999999999996,\n",
       "   0.85712500000000003,\n",
       "   0.85487500000000005,\n",
       "   0.854375,\n",
       "   0.87587499999999996,\n",
       "   0.87475000000000003,\n",
       "   0.86699999999999999],\n",
       "  [0.85662499999999997,\n",
       "   0.858375,\n",
       "   0.85624999999999996,\n",
       "   0.85962499999999997,\n",
       "   0.86987499999999995,\n",
       "   0.87137500000000001,\n",
       "   0.87612500000000004],\n",
       "  [0.85237499999999999,\n",
       "   0.84287500000000004,\n",
       "   0.84662499999999996,\n",
       "   0.84187500000000004,\n",
       "   0.85337499999999999,\n",
       "   0.863375,\n",
       "   0.85387500000000005]],\n",
       " [[0.860375,\n",
       "   0.86250000000000004,\n",
       "   0.85750000000000004,\n",
       "   0.87475000000000003,\n",
       "   0.87749999999999995,\n",
       "   0.87224999999999997],\n",
       "  [0.84162499999999996,\n",
       "   0.84075,\n",
       "   0.84424999999999994,\n",
       "   0.85362499999999997,\n",
       "   0.85912500000000003,\n",
       "   0.85924999999999996],\n",
       "  [0.83674999999999999,\n",
       "   0.83837499999999998,\n",
       "   0.83525000000000005,\n",
       "   0.84662499999999996,\n",
       "   0.86362499999999998,\n",
       "   0.84575]],\n",
       " [[0.84437499999999999,\n",
       "   0.84199999999999997,\n",
       "   0.86075000000000002,\n",
       "   0.86662499999999998,\n",
       "   0.85587500000000005],\n",
       "  [0.84712500000000002,\n",
       "   0.85037499999999999,\n",
       "   0.85587500000000005,\n",
       "   0.86324999999999996,\n",
       "   0.86287499999999995],\n",
       "  [0.84787500000000005,\n",
       "   0.84387500000000004,\n",
       "   0.856375,\n",
       "   0.86675000000000002,\n",
       "   0.85462499999999997]],\n",
       " [[0.84599999999999997,\n",
       "   0.86812500000000004,\n",
       "   0.87212500000000004,\n",
       "   0.85887500000000006],\n",
       "  [0.85262499999999997,\n",
       "   0.85962499999999997,\n",
       "   0.86787499999999995,\n",
       "   0.86875000000000002],\n",
       "  [0.85550000000000004,\n",
       "   0.86524999999999996,\n",
       "   0.87787499999999996,\n",
       "   0.86812500000000004]],\n",
       " [[0.86399999999999999, 0.86837500000000001, 0.86075000000000002],\n",
       "  [0.85275000000000001, 0.86075000000000002, 0.86275000000000002],\n",
       "  [0.87175000000000002, 0.88387499999999997, 0.87012500000000004]],\n",
       " [[0.85299999999999998, 0.84075],\n",
       "  [0.86787499999999995, 0.86787499999999995],\n",
       "  [0.88075000000000003, 0.87162499999999998]]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
